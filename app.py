import streamlit as st
import pandas as pd
import numpy as np
import os
import gc
import datetime
from scipy import stats
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression
from sklearn.preprocessing import StandardScaler
from statsmodels.stats.multitest import multipletests

# ==========================================
# 0. åŸºç¡€é…ç½®ä¸å¯¼å…¥
# ==========================================
st.set_page_config(page_title="MetaboAnalyst Pro", page_icon="ğŸ§¬", layout="wide")

# é¢œè‰²ä¸æ ·å¼é…ç½®
COLOR_PALETTE = {'Up': '#CD0000', 'Down': '#00008B', 'NS': '#E0E0E0'} 
GROUP_COLORS = ['#E64B35', '#4DBBD5', '#00A087', '#3C5488', '#F39B7F', '#8491B4', '#91D1C2', '#DC0000', '#7E6148', '#B09C85']

st.markdown("""
<style>
    .block-container {padding-top: 2rem !important; padding-bottom: 3rem !important;}
    h1, h2, h3, div, p {font-family: 'Arial', sans-serif; color: #2c3e50;}
    /* ä¼˜åŒ–æŒ‰é’®æ ·å¼ */
    button[data-baseweb="tab"] {font-weight: bold;}
    div[data-testid="stForm"] button {
        width: 100%; background-color: #ff4b4b; color: white; font-weight: bold; border: none; padding: 0.5rem;
    }
    .process-btn button {
        width: 100%; background-color: #4CAF50 !important; color: white !important; font-weight: bold; border-radius: 8px;
    }
</style>
""", unsafe_allow_html=True)

# å¯¼å…¥æœ¬åœ°æ¨¡å—
try:
    from data_preprocessing import data_cleaning_pipeline, parse_metdna_file, merge_multiple_dfs, apply_sample_info, align_sample_info
except ImportError:
    st.error("âŒ ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ° 'data_preprocessing.py'ã€‚")
    st.stop()

try:
    from serrf_module import serrf_normalization
except ImportError:
    pass

# ==========================================
# 1. ç§‘å­¦è®¡ç®—ä¸ç»˜å›¾å‡½æ•°
# ==========================================
def update_layout_square(fig, title="", x_title="", y_title="", width=600, height=600):
    fig.update_layout(
        template="simple_white", width=width, height=height,
        title={'text': title, 'y':0.95, 'x':0.5, 'xanchor': 'center'},
        xaxis=dict(title=x_title, showline=True, linewidth=2, mirror=True),
        yaxis=dict(title=y_title, showline=True, linewidth=2, mirror=True),
        legend=dict(yanchor="top", y=1, xanchor="left", x=1.15),
        margin=dict(l=80, r=180, t=80, b=80)
    )
    return fig

def get_ellipse_coordinates(x, y, std_mult=2):
    if len(x) < 3: return None, None
    mean_x, mean_y = np.mean(x), np.mean(y)
    cov = np.cov(x, y)
    vals, vecs = np.linalg.eigh(cov)
    order = vals.argsort()[::-1]
    vals, vecs = vals[order], vecs[:,order]
    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))
    width, height = 2 * std_mult * np.sqrt(vals)
    t = np.linspace(0, 2*np.pi, 100)
    ell_x = width/2 * np.cos(t)
    ell_y = height/2 * np.sin(t)
    rad = np.radians(theta)
    R = np.array([[np.cos(rad), -np.sin(rad)], [np.sin(rad), np.cos(rad)]])
    ell_coords = np.dot(R, np.array([ell_x, ell_y]))
    return ell_coords[0] + mean_x, ell_coords[1] + mean_y

def calculate_vips(model):
    t = model.x_scores_; w = model.x_weights_; q = model.y_loadings_
    p, h = w.shape; vips = np.zeros((p,))
    s = np.diag(t.T @ t @ q.T @ q); total_s = np.sum(s)
    for i in range(p):
        weight = np.array([(w[i, j] / np.linalg.norm(w[:, j]))**2 for j in range(h)])
        vips[i] = np.sqrt(p * (s @ weight) / total_s)
    return vips

@st.cache_data
def run_pairwise_statistics(df, group_col, case, control, features, equal_var=False):
    g1 = df[df[group_col] == case]
    g2 = df[df[group_col] == control]
    res = []
    for f in features:
        v1, v2 = g1[f].values, g2[f].values
        fc = np.mean(v1) - np.mean(v2) 
        try: t, p = stats.ttest_ind(v1, v2, equal_var=equal_var)
        except: p = 1.0
        if np.isnan(p): p = 1.0
        res.append({'Metabolite': f, 'Log2_FC': fc, 'P_Value': p})
    res_df = pd.DataFrame(res).dropna()
    if not res_df.empty:
        _, p_corr, _, _ = multipletests(res_df['P_Value'], method='fdr_bh')
        res_df['FDR'] = p_corr
        res_df['-Log10_P'] = -np.log10(res_df['P_Value'])
    else: res_df['FDR'] = 1.0; res_df['-Log10_P'] = 0
    return res_df

# ==========================================
# 2. ä¼šè¯çŠ¶æ€ç®¡ç† (Session State)
# ==========================================
if 'raw_df' not in st.session_state: st.session_state.raw_df = None
if 'feature_meta' not in st.session_state: st.session_state.feature_meta = None
if 'data_loaded' not in st.session_state: st.session_state.data_loaded = False
if 'qc_report' not in st.session_state: st.session_state.qc_report = {}

# ==========================================
# 3. ä¾§è¾¹æ ï¼šæ™ºèƒ½é…ç½®ä¸åŠ è½½
# ==========================================
with st.sidebar:
    st.header("ğŸ› ï¸ æ•°æ®æ§åˆ¶å°")
    
    # --- Step 1: Sample Info ä¸Šä¼ ä¸è‡ªåŠ¨æ¨æ–­ ---
    st.markdown("#### 1. ä¸Šä¼  Sample Info (SERRF å¿…é€‰)")
    sample_info_file = st.file_uploader("Sample Info (.csv/.xlsx)", type=["csv", "xlsx"], key="info")
    info_df = None
    
    if sample_info_file:
        try:
            if sample_info_file.name.endswith('.csv'): info_df = pd.read_csv(sample_info_file)
            else: info_df = pd.read_excel(sample_info_file)
            st.caption(f"âœ… å·²åŠ è½½ {len(info_df)} è¡Œæ ·æœ¬ä¿¡æ¯")
        except: st.error("æ–‡ä»¶è¯»å–å¤±è´¥")

    # --- Step 2: SERRF æ™ºèƒ½è®¾ç½® ---
    st.markdown("#### 2. SERRF æ‰¹æ¬¡æ ¡æ­£")
    use_serrf = st.checkbox("å¯ç”¨ SERRF æ ¡æ­£", value=False)
    serrf_ready = False
    
    if use_serrf:
        if info_df is not None:
            # 1. èŒƒå›´é€‰æ‹© (æ€§èƒ½å…³é”®)
            serrf_scope = st.radio("æ ¡æ­£èŒƒå›´:", ["ä»…å·²æ³¨é‡Šç‰¹å¾ (æ¨è)", "å…¨éƒ¨ç‰¹å¾"], index=0, 
                                   help="æ¨èï¼ä»…æ ¡æ­£æœ‰åå­—çš„ç‰¹å¾ï¼Œæœªæ³¨é‡Šçš„ç‰¹å¾å°†ç›´æ¥ä¸¢å¼ƒã€‚é€Ÿåº¦å¿«10å€ä¸”ä¿ç•™ç²¾åã€‚")
            
            # 2. åˆ—åè‡ªåŠ¨ä¾¦æµ‹ (Auto-Detect Logic)
            cols = list(info_df.columns)
            cols_lower = [c.lower() for c in cols]
            
            # æ‰¾ Order åˆ— (åŒ…å« order, run, idx...)
            order_candidates = [i for i, c in enumerate(cols_lower) if any(x in c for x in ['order', 'run', 'idx', 'seq'])]
            idx_order = order_candidates[0] if order_candidates else 0
            
            # æ‰¾ Type åˆ— (åŒ…å« class, type, group...)
            type_candidates = [i for i, c in enumerate(cols_lower) if any(x in c for x in ['class', 'type', 'group'])]
            # è¿›ä¸€æ­¥æ‰¾å†…å®¹é‡Œæœ‰ 'qc' çš„åˆ—
            final_type_idx = type_candidates[0] if type_candidates else 0
            found_qc_col = False
            for idx in type_candidates:
                col_name = cols[idx]
                if info_df[col_name].astype(str).str.contains('qc', case=False).any():
                    final_type_idx = idx
                    found_qc_col = True
                    break
            
            # æ‰¾ QC æ ‡ç­¾ (è‡ªåŠ¨ä» Type åˆ—é‡ŒæŠ“å–ç±»ä¼¼ QC çš„å€¼)
            default_qc_label = "QC"
            if found_qc_col:
                type_vals = info_df.iloc[:, final_type_idx].unique().astype(str)
                qc_match = next((v for v in type_vals if 'qc' in v.lower()), "QC")
                default_qc_label = qc_match

            # æ˜¾ç¤ºè®¾ç½® (é»˜è®¤å€¼å·²æ™ºèƒ½å¡«å¥½)
            c1, c2, c3 = st.columns(3)
            run_order_col = c1.selectbox("Orderåˆ—", cols, index=idx_order)
            sample_type_col = c2.selectbox("Typeåˆ—", cols, index=final_type_idx)
            qc_label = c3.text_input("QCæ ‡ç­¾", value=default_qc_label)
            serrf_ready = True
        else:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼  Sample Info")

    # --- Step 3: æ•°æ®ä¸Šä¼  ---
    st.markdown("#### 3. ä¸Šä¼  MetDNA æ•°æ®")
    uploaded_files = st.file_uploader("MetDNAæ–‡ä»¶ (æ”¯æŒå¤šé€‰)", type=["csv", "xlsx"], accept_multiple_files=True, key="data")
    
    st.markdown("---")
    
    # --- Step 4: å¯åŠ¨æŒ‰é’® ---
    process_container = st.container()
    process_container.markdown('<div class="process-btn">', unsafe_allow_html=True)
    start_process = process_container.button("ğŸ“¥ å¼€å§‹å¤„ç†æ•°æ® (Load & Process)")
    process_container.markdown('</div>', unsafe_allow_html=True)

    # ==========================================
    # æ ¸å¿ƒå¤„ç†é€»è¾‘ (ç‚¹å‡»æŒ‰é’®åè§¦å‘)
    # ==========================================
    if start_process:
        st.session_state.qc_report = {} # é‡ç½®æŠ¥å‘Š
        
        if not uploaded_files:
            st.error("è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶ï¼")
        else:
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            with st.spinner("æ­£åœ¨å¯åŠ¨é«˜æ€§èƒ½å¤„ç†å¼•æ“..."):
                parsed_results = []
                
                for i, file in enumerate(uploaded_files):
                    status_text.text(f"æ­£åœ¨å¤„ç† ({i+1}/{len(uploaded_files)}): {file.name} ...")
                    try:
                        file.seek(0)
                        file_type = 'csv' if file.name.endswith('.csv') else 'excel'
                        unique_name = f"{os.path.splitext(file.name)[0]}_{i+1}{os.path.splitext(file.name)[1]}"
                        
                        # 1. è§£æ
                        df_t, meta, err = parse_metdna_file(file, unique_name, file_type=file_type)
                        if err: 
                            st.warning(f"{file.name}: {err}"); continue
                        
                        # 2. ä¼˜åŒ–: SERRF å‰ç½®è¿‡æ»¤ (æé€Ÿæ¨¡å¼æ ¸å¿ƒ)
                        if use_serrf and serrf_ready and serrf_scope.startswith("ä»…å·²æ³¨é‡Š"):
                            annotated_ids = meta[meta['Is_Annotated'] == True].index
                            cols_to_keep = ['SampleID', 'Group'] + [c for c in df_t.columns if c in annotated_ids]
                            df_t = df_t[cols_to_keep]
                            meta = meta.loc[meta.index.isin(df_t.columns)]
                            
                        # 3. å¯¹é½ Sample Info
                        info_aligned = None
                        if info_df is not None:
                            info_aligned = align_sample_info(df_t, info_df)
                            g_col = next((c for c in info_aligned.columns if c.lower() in ['group', 'class']), None)
                            if g_col: df_t['Group'] = info_aligned[g_col].fillna(df_t['Group']).values
                        
                        # 4. æ‰§è¡Œ SERRF
                        if use_serrf and serrf_ready and info_aligned is not None:
                            # æ£€æŸ¥åŒ¹é…åº¦
                            n_matched = info_aligned[run_order_col].notna().sum()
                            if n_matched == 0:
                                st.error(f"âŒ {file.name}: æ ·æœ¬ååŒ¹é…å¤±è´¥ (0/{len(df_t)})ï¼è·³è¿‡æ ¡æ­£ã€‚")
                                st.session_state.qc_report[unique_name] = {"Status": "Failed (No Match)"}
                            else:
                                if run_order_col in info_aligned.columns and sample_type_col in info_aligned.columns:
                                    num_cols = df_t.select_dtypes(include=[np.number]).columns.tolist()
                                    df_numeric = df_t[num_cols]
                                    
                                    # è°ƒç”¨ SERRF æ¨¡å—
                                    corrected_data, serrf_stats = serrf_normalization(
                                        df_numeric, info_aligned, run_order_col, sample_type_col, qc_label
                                    )
                                    
                                    if corrected_data is not None:
                                        # 5. æ™ºèƒ½å›æ»š (Smart Revert)
                                        rsd_before = serrf_stats['RSD_Before']
                                        rsd_after = serrf_stats['RSD_After']
                                        
                                        # åªæœ‰å½“ RSD çœŸçš„å˜å¥½äº†ï¼Œæˆ–è€…åŸæœ¬å°±å¾ˆå·®ï¼Œæ‰åº”ç”¨æ ¡æ­£
                                        # å¦‚æœåŸæœ¬å¾ˆå¥½(e.g. <15%) ä¸”æ ¡æ­£åå˜å·®ï¼Œåˆ™å›æ»š
                                        if rsd_after > rsd_before:
                                            st.session_state.qc_report[unique_name] = {
                                                "Status": "Skipped (Worse)", "RSD_Before": rsd_before, "RSD_After": rsd_after
                                            }
                                            # ä¸åº”ç”¨æ ¡æ­£ï¼Œdf_t ä¿æŒåŸæ ·
                                        else:
                                            for c in corrected_data.columns: df_t[c] = corrected_data[c].values
                                            st.session_state.qc_report[unique_name] = {
                                                "Status": "Success", "RSD_Before": rsd_before, "RSD_After": rsd_after
                                            }
                                    else:
                                        st.error(f"âŒ {file.name}: SERRF è®¡ç®—å¤±è´¥ (QCä¸è¶³?)")
                                else:
                                    st.warning(f"{file.name}: åˆ—åä¸åŒ¹é…ï¼Œè·³è¿‡")

                        parsed_results.append((df_t, meta, unique_name))
                        
                        # 6. å†…å­˜å›æ”¶ (é˜²æ­¢ OOM)
                        del df_t, meta, info_aligned
                        gc.collect()

                    except Exception as e:
                        st.error(f"å¤„ç† {file.name} å¤±è´¥: {e}")
                    
                    progress_bar.progress((i + 1) / len(uploaded_files))

                if parsed_results:
                    # 7. åˆå¹¶æ•°æ®
                    if len(parsed_results) == 1:
                        st.session_state.raw_df = parsed_results[0][0]
                        st.session_state.feature_meta = parsed_results[0][1]
                    else:
                        m_df, m_meta, m_err = merge_multiple_dfs(parsed_results)
                        if m_err: st.error(m_err)
                        else:
                            st.session_state.raw_df = m_df
                            st.session_state.feature_meta = m_meta
                    
                    st.session_state.data_loaded = True
                    st.success("âœ… æ•°æ®åŠ è½½ä¸å¤„ç†å®Œæˆï¼")
                    st.rerun() # å¼ºåˆ¶åˆ·æ–°é¡µé¢ä»¥æ›´æ–°ä¸»é¢æ¿
                else:
                    st.error("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡ä»¶")

    # --- Step 5: å¸¸é©»ä¸‹è½½åŒº ---
    if st.session_state.data_loaded and st.session_state.raw_df is not None:
        raw_df = st.session_state.raw_df
        st.info(f"å½“å‰æ•°æ®: {len(raw_df)} æ ·æœ¬ x {len(raw_df.columns)-2} ç‰¹å¾")
        
        # å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M")
        csv_data = raw_df.to_csv(index=False).encode('utf-8')
        st.download_button("ğŸ“¥ å¯¼å‡ºåˆå¹¶æ•°æ®", csv_data, f"Metabo_Processed_{ts}.csv", "text/csv")
        st.divider()

        # --- Step 6: ç»Ÿè®¡åˆ†æè¡¨å• ---
        with st.form(key='analysis_form'):
            st.markdown("### âš™ï¸ ç»Ÿè®¡åˆ†æå‚æ•°")
            
            non_num = raw_df.select_dtypes(exclude=[np.number]).columns.tolist()
            default_grp_idx = non_num.index('Group') if 'Group' in non_num else 0
            group_col = st.selectbox("åˆ†ç»„åˆ—", non_num, index=default_grp_idx)
            
            filter_option = st.radio("ç»Ÿè®¡èŒƒå›´:", ["å…¨éƒ¨ç‰¹å¾", "ä»…å·²æ³¨é‡Šç‰¹å¾"], index=0)
            
            with st.expander("æ•°æ®æ¸…æ´—ä¸å½’ä¸€åŒ– (é«˜çº§)", expanded=False):
                miss_th = st.slider("å‰”é™¤ç¼ºå¤±ç‡ > X", 0.0, 1.0, 0.5, 0.1)
                impute_m = st.selectbox("å¡«å……æ–¹æ³•", ["min", "mean", "zero"], index=0)
                # æ–°å¢ PQN
                norm_m = st.selectbox("æ ·æœ¬å½’ä¸€åŒ–", ["None", "PQN", "Sum", "Median"], index=1, help="PQN (æ¦‚ç‡å•†å½’ä¸€åŒ–) æ˜¯ä»£è°¢ç»„å­¦é‡‘æ ‡å‡†")
                do_log = st.checkbox("Log2 è½¬åŒ–", value=True)
                scale_m = st.selectbox("ç‰¹å¾ç¼©æ”¾", ["None", "Auto", "Pareto"], index=2, help="Pareto é€‚åˆå¤§å¤šæ•°ä»£è°¢ç»„æ•°æ®")

            current_groups = sorted(raw_df[group_col].astype(str).unique())
            st.markdown("### ç»„åˆ«å¯¹æ¯”")
            selected_groups = st.multiselect("çº³å…¥ç»„:", current_groups, default=current_groups[:2] if len(current_groups)>=2 else current_groups)
            
            c1, c2 = st.columns(2)
            valid_grps_list = list(selected_groups)
            case_grp = c1.selectbox("Exp (Case)", valid_grps_list, index=0 if valid_grps_list else None)
            ctrl_grp = c2.selectbox("Ctrl (Ref)", valid_grps_list, index=1 if len(valid_grps_list)>1 else 0)
            
            c3, c4 = st.columns(2)
            p_th = c3.number_input("P-value", 0.05, format="%.3f")
            fc_th = c4.number_input("Log2 FC", 1.0)
            use_equal_var = st.checkbox("Student's t-test (Equal Var)", value=True)
            enable_jitter = st.checkbox("ç«å±±å›¾æŠ–åŠ¨", value=True)
            
            st.markdown("---")
            submit_button = st.form_submit_button(label='ğŸš€ è¿è¡Œç»Ÿè®¡åˆ†æ (Run Stats)')

# ==========================================
# 4. ä¸»é¢æ¿å±•ç¤ºåŒº
# ==========================================

# çŠ¶æ€1: ç­‰å¾…ä¸Šä¼ 
if not st.session_state.data_loaded:
    st.title("ğŸ§¬ MetaboAnalyst Pro")
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ æ•°æ®å¹¶ç‚¹å‡» **â€œå¼€å§‹å¤„ç†æ•°æ®â€** æŒ‰é’®ã€‚")
    st.markdown("""
    ### å¿«é€ŸæŒ‡å—
    1. **æ ·æœ¬ä¿¡æ¯**ï¼šä¸Šä¼ å« Order å’Œ Type çš„è¡¨æ ¼ï¼ˆSERRF å¿…é€‰ï¼‰ã€‚
    2. **æ•°æ®æ–‡ä»¶**ï¼šæ”¯æŒå¤šæ¨¡å¼æ–‡ä»¶æ‰¹é‡ä¸Šä¼ ï¼ˆå¦‚ RP+, HILIC-ï¼‰ã€‚
    3. **è‡ªåŠ¨å¤„ç†**ï¼šå‹¾é€‰ SERRFï¼Œç‚¹å‡»ç»¿è‰²æŒ‰é’®ï¼Œç¨‹åºè‡ªåŠ¨æ¸…æ´—ã€æ ¡æ­£ã€åˆå¹¶ã€‚
    4. **ç»Ÿè®¡åˆ†æ**ï¼šæ•°æ®åŠ è½½åï¼Œé…ç½®å‚æ•°è¿è¡Œ PCA/ç«å±±å›¾/çƒ­å›¾ã€‚
    """)
    st.stop()

# çŠ¶æ€2: æ•°æ®åŠ è½½å®Œæˆï¼Œæ˜¾ç¤º SERRF æŠ¥å‘Š
if not submit_button:
    st.title("âœ… æ•°æ®å‡†å¤‡å°±ç»ª")
    
    if st.session_state.qc_report:
        st.subheader("ğŸ” SERRF æ ¡æ­£æ•ˆæœè¯„ä¼°")
        cols = st.columns(len(st.session_state.qc_report))
        for idx, (fname, report) in enumerate(st.session_state.qc_report.items()):
            with cols[idx % 3]:
                r_before = report.get('RSD_Before', 0)
                r_after = report.get('RSD_After', 0)
                delta = r_after - r_before
                
                if report['Status'] == 'Success':
                    st.success(f"ğŸ“„ {fname}")
                    st.metric("QC RSD", f"{r_after:.1f}%", f"{delta:.1f}%", delta_color="inverse")
                    st.caption(f"ä¼˜åŒ–å‰: {r_before:.1f}%")
                elif report['Status'] == 'Skipped (Worse)':
                    st.warning(f"ğŸ“„ {fname}")
                    st.metric("QC RSD (å·²å›æ»š)", f"{r_before:.1f}%", f"æ ¡æ­£å: {r_after:.1f}% (+{delta:.1f}%)", delta_color="off")
                    st.caption("æ£€æµ‹åˆ°æ ¡æ­£åè´¨é‡ä¸‹é™ï¼Œå·²ä¿ç•™åŸæ•°æ®ã€‚")
                else:
                    st.error(f"ğŸ“„ {fname}: {report['Status']}")
    
    st.markdown("---")
    st.markdown("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§é…ç½®å‚æ•°å¹¶ç‚¹å‡» **â€œè¿è¡Œç»Ÿè®¡åˆ†æâ€**ã€‚")
    st.subheader("åˆå¹¶åæ•°æ®é¢„è§ˆ")
    st.dataframe(st.session_state.raw_df.head(50))
    st.stop()

# çŠ¶æ€3: è¿è¡Œç»Ÿè®¡åˆ†æ
if submit_button:
    if len(selected_groups) < 2:
        st.error("è¯·è‡³å°‘é€‰æ‹© 2 ä¸ªç»„ï¼"); st.stop()

    with st.spinner("æ­£åœ¨è¿›è¡Œç»Ÿè®¡åˆ†æä¸ç»˜å›¾ (WebGLåŠ é€Ÿä¸­)..."):
        raw_df = st.session_state.raw_df
        feature_meta = st.session_state.feature_meta

        # æ•°æ®æ¸…æ´—ç®¡é“
        df_proc, feats = data_cleaning_pipeline(
            raw_df, group_col, missing_thresh=miss_th, impute_method=impute_m, 
            norm_method=norm_m, log_transform=do_log, scale_method=scale_m
        )

        if filter_option == "ä»…å·²æ³¨é‡Šç‰¹å¾":
            if feature_meta is not None:
                annotated_feats = feature_meta[feature_meta['Is_Annotated'] == True].index.tolist()
                feats = [f for f in feats if f in annotated_feats]
                if not feats: st.error("è¿‡æ»¤åæ— ç‰¹å¾ï¼"); st.stop()
            else: st.warning("é MetDNA æ•°æ®ï¼Œæ— æ³•è¿‡æ»¤ã€‚")

        df_sub = df_proc[df_proc[group_col].isin(selected_groups)].copy()

        # ç»Ÿè®¡åˆ†æ
        if case_grp != ctrl_grp:
            res_stats = run_pairwise_statistics(df_sub, group_col, case_grp, ctrl_grp, feats, equal_var=use_equal_var)
            if feature_meta is not None:
                res_stats = res_stats.merge(feature_meta[['Confidence_Level', 'Clean_Name']], 
                                            left_on='Metabolite', right_index=True, how='left')
                res_stats['Confidence_Level'] = res_stats['Confidence_Level'].fillna('Unknown')
            else: res_stats['Confidence_Level'] = 'N/A'
            
            res_stats['Sig'] = 'NS'
            res_stats.loc[(res_stats['P_Value'] < p_th) & (res_stats['Log2_FC'] > fc_th), 'Sig'] = 'Up'
            res_stats.loc[(res_stats['P_Value'] < p_th) & (res_stats['Log2_FC'] < -fc_th), 'Sig'] = 'Down'
            sig_metabolites = res_stats[res_stats['Sig'] != 'NS']['Metabolite'].tolist()
        else:
            res_stats = pd.DataFrame(); sig_metabolites = []

        # ====================
        # ç»“æœå¯è§†åŒ–åŒº
        # ====================
        st.title("ğŸ“Š ä»£è°¢ç»„å­¦åˆ†ææŠ¥å‘Š")
        st.caption(f"å¯¹æ¯”: {case_grp} vs {ctrl_grp} | ç‰¹å¾æ•°: {len(feats)} | Scaling: {scale_m} | Norm: {norm_m}")

        # QC Check
        qc_mask = df_sub[group_col].astype(str).str.contains('QC', case=False)
        if qc_mask.sum() >= 2:
             with st.expander("ğŸ” è´¨é‡æ§åˆ¶ (QC Quality Check)", expanded=True):
                 qc_data = df_sub.loc[qc_mask, feats]
                 qc_rsd = (qc_data.std() / qc_data.mean()) * 100
                 median_rsd = qc_rsd.median()
                 
                 c1, c2 = st.columns([1, 3])
                 c1.metric("QC Median RSD", f"{median_rsd:.1f}%")
                 fig_rsd = px.histogram(qc_rsd, nbins=50, title="QC RSD Distribution", width=600, height=300)
                 fig_rsd.add_vline(x=20, line_dash="dash", line_color="green")
                 c2.plotly_chart(fig_rsd, use_container_width=True)

        tabs = st.tabs(["ğŸ“Š PCA", "ğŸ¯ PLS-DA", "â­ VIP ç‰¹å¾", "ğŸŒ‹ ç«å±±å›¾", "ğŸ”¥ çƒ­å›¾", "ğŸ“‘ è¯¦æƒ…"])

        # PCA
        with tabs[0]:
            c1, c2 = st.columns([1, 2])
            with c2:
                if len(df_sub) < 3: st.warning("æ ·æœ¬ä¸è¶³")
                else:
                    X = StandardScaler().fit_transform(df_sub[feats])
                    pca = PCA(n_components=2).fit(X)
                    pcs = pca.transform(X)
                    var = pca.explained_variance_ratio_
                    fig_pca = px.scatter(x=pcs[:,0], y=pcs[:,1], color=df_sub[group_col], symbol=df_sub[group_col],
                                         color_discrete_sequence=GROUP_COLORS, width=600, height=600, render_mode='webgl')
                    fig_pca.update_traces(marker=dict(size=14, line=dict(width=1, color='black'), opacity=0.9))
                    update_layout_square(fig_pca, "PCA Score Plot", f"PC1 ({var[0]:.1%})", f"PC2 ({var[1]:.1%})")
                    st.plotly_chart(fig_pca, use_container_width=False)

        # PLS-DA
        with tabs[1]:
            c1, c2 = st.columns([1, 2])
            with c2:
                if len(df_sub) < 3: st.warning("æ ·æœ¬ä¸è¶³")
                else:
                    X_pls = df_sub[feats].values
                    y_labels = pd.factorize(df_sub[group_col])[0]
                    pls_model = PLSRegression(n_components=2).fit(X_pls, y_labels)
                    plot_df = pd.DataFrame({'C1': pls_model.x_scores_[:,0], 'C2': pls_model.x_scores_[:,1], 'Group': df_sub[group_col].values})
                    fig_pls = px.scatter(plot_df, x='C1', y='C2', color='Group', symbol='Group',
                                         color_discrete_sequence=GROUP_COLORS, width=600, height=600, render_mode='webgl')
                    for i, grp in enumerate(selected_groups):
                        sub_g = plot_df[plot_df['Group'] == grp]
                        if len(sub_g) >= 3:
                            ell_x, ell_y = get_ellipse_coordinates(sub_g['C1'], sub_g['C2'])
                            if ell_x is not None: fig_pls.add_trace(go.Scatter(x=ell_x, y=ell_y, mode='lines', line=dict(color=GROUP_COLORS[i%len(GROUP_COLORS)], width=2, dash='dash'), showlegend=False, hoverinfo='skip'))
                    fig_pls.update_traces(marker=dict(size=14, line=dict(width=1.5, color='black'), opacity=1.0))
                    update_layout_square(fig_pls, "PLS-DA Score Plot", "Component 1", "Component 2")
                    st.plotly_chart(fig_pls, use_container_width=False)

        # VIP
        with tabs[2]:
            st.markdown("### Top 25 VIP Features")
            if 'pls_model' in locals():
                vip_scores = calculate_vips(pls_model)
                vip_df = pd.DataFrame({'Metabolite': feats, 'VIP': vip_scores})
                if feature_meta is not None:
                     vip_df = vip_df.merge(feature_meta[['Clean_Name']], left_on='Metabolite', right_index=True, how='left')
                     vip_df['Display_Name'] = vip_df['Clean_Name'].fillna(vip_df['Metabolite'])
                else: vip_df['Display_Name'] = vip_df['Metabolite']
                top_vip = vip_df.sort_values('VIP', ascending=True).tail(25)
                c1, c2 = st.columns([1, 6])
                with c2:
                    fig_vip = px.bar(top_vip, x="VIP", y="Display_Name", orientation='h',
                                     color="VIP", color_continuous_scale="RdBu_r", width=800, height=700)
                    fig_vip.add_vline(x=1.0, line_dash="dash", line_color="black")
                    fig_vip.update_layout(template="simple_white", width=800, height=700, title={'text': "VIP Scores", 'x':0.5, 'xanchor': 'center'}, coloraxis_showscale=False)
                    st.plotly_chart(fig_vip, use_container_width=False)

        # Volcano
        with tabs[3]:
            c1, c2 = st.columns([1, 2])
            with c2:
                plot_df = res_stats.copy()
                x_c, y_c = "Log2_FC", "-Log10_P"
                if enable_jitter:
                    np.random.seed(42)
                    xr, yr = (plot_df[x_c].max()-plot_df[x_c].min()) or 1, (plot_df[y_c].max()-plot_df[y_c].min()) or 1
                    plot_df['Log2_FC_J'] = plot_df[x_c] + np.random.normal(0, xr*0.015, len(plot_df))
                    plot_df['-Log10_P_J'] = plot_df[y_c] + np.random.normal(0, yr*0.015, len(plot_df))
                    x_c, y_c = "Log2_FC_J", "-Log10_P_J"
                
                fig_vol = px.scatter(plot_df, x=x_c, y=y_c, color="Sig", color_discrete_map=COLOR_PALETTE,
                                     hover_data={"Metabolite":True}, width=600, height=600, render_mode='webgl')
                fig_vol.add_hline(y=-np.log10(p_th), line_dash="dash", line_color="black"); fig_vol.add_vline(x=fc_th, line_dash="dash", line_color="black"); fig_vol.add_vline(x=-fc_th, line_dash="dash", line_color="black")
                update_layout_square(fig_vol, "Volcano Plot", "Log2 Fold Change", "-Log10(P-value)")
                st.plotly_chart(fig_vol, use_container_width=False)

        # Heatmap
        with tabs[4]:
            if not sig_metabolites: st.info("æ— æ˜¾è‘—å·®å¼‚ç‰©")
            else:
                c1, c2 = st.columns([1, 6])
                with c2:
                    top_n = 50
                    top_feats = res_stats.sort_values('P_Value').head(top_n)['Metabolite'].tolist()
                    hm_data = df_sub.set_index(group_col)[top_feats].T 
                    sample_groups = df_sub[group_col]
                    lut = {grp: GROUP_COLORS[i % len(GROUP_COLORS)] for i, grp in enumerate(sample_groups.unique())}
                    col_colors = sample_groups.map(lut)
                    if feature_meta is not None:
                        hm_data.index = [feature_meta.loc[f, 'Clean_Name'] if f in feature_meta.index else f for f in hm_data.index]
                    try:
                        g = sns.clustermap(hm_data.astype(float), z_score=0, cmap="vlag", center=0, 
                                           col_colors=col_colors, figsize=(12, 14), 
                                           dendrogram_ratio=(.1, .1), 
                                           cbar_pos=(0.35, 0.96, 0.3, 0.02), cbar_kws={'orientation': 'horizontal', 'label': 'Z-Score'})
                        g.ax_heatmap.set_ylabel(""); g.ax_heatmap.set_xlabel("")
                        g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xmajorticklabels(), rotation=90, fontsize=9)
                        st.pyplot(g.fig)
                    except Exception as e: st.error(f"ç»˜å›¾é”™è¯¯: {e}")

        # Details
        with tabs[5]:
            c1, c2 = st.columns([1.5, 1])
            with c1:
                st.subheader("ç»Ÿè®¡è¡¨")
                if not res_stats.empty:
                    display_df = res_stats.sort_values("P_Value").copy()
                    if 'Clean_Name' in display_df.columns: display_df['Name'] = display_df['Clean_Name'].fillna(display_df['Metabolite'])
                    else: display_df['Name'] = display_df['Metabolite']
                    st.dataframe(display_df[[c for c in ["Name", "Log2_FC", "P_Value", "FDR", "Confidence_Level"] if c in display_df]].style.format({"Log2_FC": "{:.2f}", "P_Value": "{:.2e}", "FDR": "{:.2e}"}).background_gradient(subset=['P_Value'], cmap="Reds_r", vmin=0, vmax=0.05), use_container_width=True, height=600)
            with c2:
                st.subheader("ç®±çº¿å›¾")
                feat_options = sorted(feats)
                def_ix = feat_options.index(sig_metabolites[0]) if sig_metabolites else 0
                target_feat = st.selectbox("é€‰æ‹©ä»£è°¢ç‰©", feat_options, index=def_ix)
                if target_feat:
                    box_df = df_sub[[group_col, target_feat]].copy()
                    fig_box = px.box(box_df, x=group_col, y=target_feat, color=group_col, color_discrete_sequence=GROUP_COLORS, points="all", width=500, height=500)
                    update_layout_square(fig_box, target_feat, "Group", "Log2 Intensity", width=500, height=500)
                    st.plotly_chart(fig_box, use_container_width=False)
